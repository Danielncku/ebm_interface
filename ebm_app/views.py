
from django.shortcuts import render
from django.http import HttpResponse,  JsonResponse
from .ml_models import MLInterpretModel
import os
os.environ["OPENAI_BASE_URL"] = "http://192.168.63.184:11434/v1"
os.environ["OPENAI_API_KEY"] = "ollama"
    
try:
    from t2ebm.llm import openai_setup
    from t2ebm import describe_graph
    
    # ç”¨ Ollama å•Ÿå‹•
    llm = openai_setup(
        model="gpt-oss:20b",   # ä½ çš„ Ollama æ¨¡å‹
        
        base_url="http://192.168.63.184:11434/v1"
    )
    
    TALK_TO_EBM_AVAILABLE = True
    print("âœ… TalkToEBM å·²ä½¿ç”¨ Ollama å•Ÿå‹•")

except Exception as e:
    TALK_TO_EBM_AVAILABLE = False
    llm = None
    describe_graph = None
    print(f"âŒ TalkToEBM åˆå§‹åŒ–å¤±æ•—ï¼ˆå·²ç¦ç”¨ï¼‰: {e}")
    
# åˆå§‹åŒ–æ¨¡å‹
feature_cols = ["Sex", "DM", "HTN", "CAD", "Age", "Pre_HD_SBP", "HR", "RR", "blood-speed",
                "Dialysis-blood-temp", "Dialysis-blood-rate", "start-weight", "Mean_BP",
                "HR_Mean_BP", "UF_BW_perc", "é€ææ¶² Ca", "é«”æº«_New", "é ä¼°è„«æ°´é‡",
                "éœè„ˆå£“(mmHg)", "é€ææ¶²å£“(mmHg)", 'idh_count_last_28d']
target_col = "Nadir90/100"

#patient data
# ml_model = MLInterpretModel("EBM_28.joblib", "Patient5.csv", feature_cols, target_col)
# API data
ml_model = MLInterpretModel("EBM_28.joblib", "interface/data/temp.csv", feature_cols, target_col)

# é¦–é 
def home_view(request):
    data = ml_model.data
    patient_list = data['ID'].unique().tolist()
    return render(request, 'ebm_app/home.html', {
        'patient_list': patient_list,
        'feature_cols': ml_model.feature_cols
    })

# Dashboard é é¢
def dashboard_view(request, patient_id):
    data = ml_model.data
    patient_list = data['ID'].unique().tolist()
    feature_cols = ml_model.feature_cols
    return render(request, 'ebm_app/dashboard.html', {
        'patient_id': patient_id,
        'patient_list': patient_list,
        'feature_cols': feature_cols
    })


# å…¨åŸŸè§£é‡‹ AJAX
def ajax_global_explanation(request):
    feature = request.GET.get('feature', None)
    if feature == '':
        feature = None
    
    # ğŸ†• æ–°å¢ï¼šè®€å–å¯†åº¦è¦–çª—åƒæ•¸
    density_enabled = request.GET.get('density_window', 'false').lower() == 'true'
    lower_percentile = float(request.GET.get('lower_percentile', 2.5))
    upper_percentile = float(request.GET.get('upper_percentile', 97.5))
    
    # ğŸ†• æ–°å¢ï¼šå‚³éåƒæ•¸çµ¦ ml_model
    html = ml_model.get_global_explanation_html(
        feature=feature,
        density_window=density_enabled,
        lower_percentile=lower_percentile,
        upper_percentile=upper_percentile
    )
    return HttpResponse(html)

# âœ… ä¿®æ”¹ï¼šå€åŸŸè§£é‡‹ AJAX åŠ å…¥é¡¯ç¤ºæ¨¡å¼åƒæ•¸
# å€åŸŸè§£é‡‹ AJAX
def ajax_local_explanation(request, patient_id):
    # ğŸ†• æ–°å¢ï¼šè®€å–é¡¯ç¤ºæ¨¡å¼åƒæ•¸
    display_mode = request.GET.get('display_mode', 'all')
    html = ml_model.get_local_explanation_html(patient_id, display_mode)
    return HttpResponse(html)


# ============================================
# âœ… åŠ åœ¨ views.py çš„æœ€åº•éƒ¨
# ============================================

def ajax_ai_explain_feature(request):
    """
    æä¾›å–®ä¸€ç‰¹å¾µçš„ AI è‡ªç„¶èªè¨€è§£é‡‹
    """
    if not TALK_TO_EBM_AVAILABLE:
        return JsonResponse({
            'success': False,
            'error': 'TalkToEBM æœªå®‰è£'
        })
    
    feature_name = request.GET.get('feature')
    patient_id = request.GET.get('patient_id', None)
    
    if not feature_name:
        return JsonResponse({
            'success': False,
            'error': 'è«‹æä¾›ç‰¹å¾µåç¨±'
        })
    
    try:
        # æª¢æŸ¥ç‰¹å¾µæ˜¯å¦å­˜åœ¨
        if feature_name not in ml_model.feature_cols:
            return JsonResponse({
                'success': False,
                'error': f'ç‰¹å¾µ {feature_name} ä¸å­˜åœ¨'
            })
        
        feature_idx = ml_model.feature_cols.index(feature_name)
        
        # æº–å‚™å®¢è£½åŒ– prompt
        custom_prompt = None
        patient_value = None
        
        if patient_id:
            patient_data = ml_model.data[ml_model.data['ID'] == str(patient_id)]
            if not patient_data.empty:
                patient_value = patient_data[feature_name].iloc[0]
                
                custom_prompt = (
                    f"è§’è‰²ï¼šä½ æ˜¯å°ˆæ¥­çš„é€æé†«ç™‚é¡§å•ã€‚\n"
                    f"èƒŒæ™¯ï¼šç—…äººçš„ {feature_name} = {patient_value:.2f}\n"
                    f"ä»»å‹™ï¼š\n"
                    f"1. ç”¨ç°¡å–®çš„è©±èªªæ˜é€™å€‹æ•¸å€¼çš„æ„ç¾©\n"
                    f"2. è§£é‡‹å®ƒå¦‚ä½•å½±éŸ¿ä½è¡€å£“é¢¨éšª\n"
                    f"3. çµ¦ä¸€å€‹å…·é«”å»ºè­°\n"
                    f"è¦æ±‚ï¼š\n"
                    f"- ä¸ç”¨å°ˆæ¥­è¡“èª\n"
                    f"- èªæ°£æº«å’Œé¼“å‹µ\n"
                    f"- ä¸è¶…é 150 å­—\n"
                )
        
        # å‘¼å« TalkToEBM
        print(f"æ­£åœ¨ç‚ºç‰¹å¾µ {feature_name} ç”Ÿæˆ AI è§£é‡‹...")
        
        description = describe_graph(
            llm,
            ml_model.model,
            feature_index=feature_idx,
            num_sentences=1,
            max_chars=30,
            style="patient",
            temperature=0.7,
            custom_prompt=custom_prompt
        )
        
        print(f"âœ… AI è§£é‡‹ç”Ÿæˆå®Œæˆ")
        
        return JsonResponse({
            'success': True,
            'explanation': description,
            'feature': feature_name,
            'patient_value': float(patient_value) if patient_value is not None else None
        })
        
    except Exception as e:
        import traceback
        print(f"âŒ AI è§£é‡‹å¤±æ•—: {e}")
        print(traceback.format_exc())
        return JsonResponse({
            'success': False,
            'error': str(e)
        })